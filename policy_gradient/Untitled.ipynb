{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "class shortGridWorld():\n",
    "    def __init__(self, start_state):\n",
    "        self.state = start_state\n",
    "        self.reward = -1\n",
    "    \n",
    "    def take_action(self, state, action):\n",
    "        if state == 0:\n",
    "            if all(action == np.array([[0.], [1.]])):\n",
    "                self.state = 0\n",
    "            else:\n",
    "                self.state = 1\n",
    "        elif state == 1:\n",
    "            if all(action == np.array([[0.], [1.]])):\n",
    "                self.state = 2\n",
    "            else:\n",
    "                self.state = 1\n",
    "        elif state == 2:\n",
    "            if all(action ==np.array([[0.],[1.]])):\n",
    "                self.state = 1\n",
    "            else:\n",
    "                self.state = 3\n",
    "                \n",
    "        if state!=3:\n",
    "            self.reward = -1\n",
    "        else:\n",
    "            self.reward = 0  \n",
    "            \n",
    "    def is_terminal(self):\n",
    "        return self.state == 3        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "class REINFORCE_mc():\n",
    "    def __init__(self, Problem, num_episodes=100, y=0.9):\n",
    "        self.Problem = Problem\n",
    "        self.num_episodes = num_episodes\n",
    "        self.y = y\n",
    "        self.theta = np.zeros((2,1))\n",
    "        self.x = np.array([[1.,0.],[0.,1.]])\n",
    "        self.r = []\n",
    "        self.s = []\n",
    "        self.a = []\n",
    "    \n",
    "    def get_policy_distribution(self):\n",
    "        return (np.exp(self.theta.T @ self.x)/np.sum(np.exp(self.theta.T @ self.x))).T\n",
    "    \n",
    "    def sample_from_distribution(self, P):\n",
    "        CP = np.cumsum(P)\n",
    "        rn = np.random.uniform()\n",
    "\n",
    "        for i in range(len(CP)):\n",
    "            if CP[i] >= rn:\n",
    "                return i\n",
    "    \n",
    "    def get_action(self):\n",
    "        action = np.zeros((2,1))\n",
    "        action[self.sample_from_distribution(self.get_policy_distribution())] = 1 # maybe a separate functon to get the feature vector of action\n",
    "        return action\n",
    "    \n",
    "    def clear(self):\n",
    "        self.r = []\n",
    "        self.s = []\n",
    "        self.a = []\n",
    "        \n",
    "    def gen_episode(self,s0=0, max_len=1000):\n",
    "        self.clear()\n",
    "        problem = self.Problem(s0)\n",
    "        \n",
    "        state = s0\n",
    "        self.s.append(state)\n",
    "        \n",
    "        G = 0\n",
    "        I = 1\n",
    "        \n",
    "        while (not problem.is_terminal()) and max_len != 0:\n",
    "            action = self.get_action()\n",
    "            self.a.append(np.argmax(action))\n",
    "            \n",
    "            problem.take_action(state, action)\n",
    "            state = problem.state\n",
    "            self.s.append(state)\n",
    "            \n",
    "            self.r.append(problem.reward)\n",
    "            G += problem.reward * I\n",
    "            I *= self.y\n",
    "            \n",
    "            max_len -= 1\n",
    "        \n",
    "        return G\n",
    "    \n",
    "    def mc_without_baseline(self):\n",
    "        for i in tqdm(range(self.num_episodes)):\n",
    "            alpha = 0.0001\n",
    "            G = self.gen_episode(0)\n",
    "            sub = 0\n",
    "            I = 1\n",
    "            \n",
    "            for t in range(len(self.s)-1):\n",
    "                G -= sub\n",
    "                delta_ln = np.copy(self.x[:,self.a[0], np.newaxis])\n",
    "                pi = self.get_policy_distribution()\n",
    "                for b in range(2):\n",
    "                    delta_ln -= np.reshape(pi[b]*self.x[:,b], (2,1))\n",
    "    \n",
    "                \n",
    "                # y^t is covered in G in this implementation. G is not strictly G, rather y^t*G\n",
    "                self.theta = self.theta + alpha*G*delta_ln \n",
    "                \n",
    "                sub = self.r[t]*I\n",
    "                I *= self.y\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = REINFORCE_mc(shortGridWorld, num_episodes=1000, y=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c08f3587bb84a4f8e29e80c5a7035da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "a.mc_without_baseline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.59061559],\n",
       "       [0.40938441]])"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.get_policy_distribution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
